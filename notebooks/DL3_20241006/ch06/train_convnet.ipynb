{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##합성곱 /플링 계층 구현 실습\n",
        "\n",
        "풀링 pooling 피처맵을 대표하는 값으로 sub sampling 대표값을 취함 \n",
        "합성곱층이랑 풀링을 비교하면\n",
        "\n",
        "합성곱은 가중치와 편향학습을 학습 - 풀링은 단순계산\n",
        "컬러채널 없음 - 독립연산\n",
        "\n",
        "***풀링은 전역적 특징을 찾아낼 수 있게 도와줌. 파라미터 숫자를 줄여 계산비용 감소 오버피팅도 억제!\n",
        "\n",
        "\n",
        "![alt text](image-2.png)\n",
        "![alt text](image-3.png)\n",
        "![alt text](image-4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 실습 - simple_convent.py 작성 > 여기서 SimpleConvNet import\n",
        "\n",
        "파일 내에서 한번 봐야할 부분~ 코드를 다 칠필욘 없고 구성에 대한 이해만!\n",
        "![alt text](image-5.png)\n",
        "![alt text](image-6.png)\n",
        "![alt text](image-7.png)\n",
        "![alt text](image-8.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1720232461098
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 모든 모듈 임포트 성공했습니다!\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# [1] 현재 노트북 폴더의 절대 경로를 기준으로 합니다.\n",
        "current_dir = os.getcwd() \n",
        "\n",
        "# [2] 'DL-Excersize' (최상위 루트) 경로를 계산합니다.\n",
        "# (ch06 -> DL3... -> notebooks -> DL-Excersize)\n",
        "# os.path.join을 사용하여 세 번 상위 폴더로 이동합니다.\n",
        "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..', '..'))\n",
        "\n",
        "# [3] 최상위 경로를 sys.path에 추가하여 'dataset'과 'common' 폴더를 찾게 합니다.\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "    print(f\"프로젝트 루트 경로 추가됨: {project_root}\")\n",
        "\n",
        "# [4] (선택적) 현재 폴더(ch06)를 추가하여 simple_convnet도 찾게 합니다.\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.append(current_dir)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# 이제 파이썬은 project_root 경로에서 'dataset' 폴더를 찾아 들어갑니다.\n",
        "try:\n",
        "    from dataset.mnist import load_mnist\n",
        "    from simple_convnet import SimpleConvNet # 같은 폴더의 파일\n",
        "    from common.trainer import Trainer \n",
        "\n",
        "    print(\"✅ 모든 모듈 임포트 성공했습니다!\")\n",
        "\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"❌ 임포트 실패: {e}\")\n",
        "    print(\"경로가 꼬였을 수 있으니, 위에 출력된 '프로젝트 루트 경로'를 확인해주세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:2.281270385679471\n",
            "=== epoch:1, train acc:0.186, test acc:0.169 ===\n",
            "train loss:2.1609534404372917\n",
            "train loss:1.8523590323603647\n",
            "train loss:1.8325024681956077\n",
            "train loss:1.3582331433245056\n",
            "train loss:0.9198793907293684\n",
            "train loss:0.8400708626339122\n",
            "train loss:0.7419010686984018\n",
            "train loss:0.5610017871451034\n",
            "train loss:0.9311018376548036\n",
            "train loss:0.9672234173395092\n",
            "train loss:0.5339494274131905\n",
            "train loss:0.4622761029525556\n",
            "train loss:0.5487598685005826\n",
            "train loss:0.5411265248583067\n",
            "train loss:0.5243157904556773\n",
            "train loss:0.46255377045211227\n",
            "train loss:0.5286657442119442\n",
            "train loss:0.34820969587234335\n",
            "train loss:0.41871020247511814\n",
            "train loss:0.5486861545901871\n",
            "train loss:0.5754047867229708\n",
            "train loss:0.43174703260416947\n",
            "train loss:0.374141267171634\n",
            "train loss:0.3121840838316375\n",
            "train loss:0.25517574391720116\n",
            "train loss:0.26893958206425694\n",
            "train loss:0.23696836965306417\n",
            "train loss:0.21514931621077527\n",
            "train loss:0.2210580203532755\n",
            "train loss:0.23846848218996722\n",
            "train loss:0.149033228759064\n",
            "train loss:0.2261982833950514\n",
            "train loss:0.12657570262346238\n",
            "train loss:0.22159130567641638\n",
            "train loss:0.28206630264484656\n",
            "train loss:0.21788800185986232\n",
            "train loss:0.22367535396790486\n",
            "train loss:0.15344514090902733\n",
            "train loss:0.14382080561307264\n",
            "train loss:0.1955043526570433\n",
            "train loss:0.2518695004723229\n",
            "train loss:0.10276200421343655\n",
            "train loss:0.10684865124868953\n",
            "train loss:0.11803741401770067\n",
            "train loss:0.2102153272903875\n",
            "train loss:0.1676157930379023\n",
            "train loss:0.1163661754996139\n",
            "train loss:0.14455740397883954\n",
            "train loss:0.10907091956200286\n",
            "train loss:0.158430393633558\n",
            "=== epoch:2, train acc:0.952, test acc:0.93 ===\n",
            "train loss:0.15072345772565698\n",
            "train loss:0.12436973837495663\n",
            "train loss:0.12896068314554252\n",
            "train loss:0.26926813471409433\n",
            "train loss:0.11738204467279129\n",
            "train loss:0.10346986005068913\n",
            "train loss:0.20125803818956808\n",
            "train loss:0.037865379922935524\n",
            "train loss:0.1344651161713608\n",
            "train loss:0.083809430768998\n",
            "train loss:0.10937968220017125\n",
            "train loss:0.07397700864034866\n",
            "train loss:0.03773759783288817\n",
            "train loss:0.08976927878162463\n",
            "train loss:0.10926544143556004\n",
            "train loss:0.11392459342738849\n",
            "train loss:0.09944791576575078\n",
            "train loss:0.07329067449295096\n",
            "train loss:0.07774899657879483\n",
            "train loss:0.04079709960501306\n",
            "train loss:0.14520639769482263\n",
            "train loss:0.07580335525886515\n",
            "train loss:0.08466679453388073\n",
            "train loss:0.06731766216629811\n",
            "train loss:0.04299564122124921\n",
            "train loss:0.04191051397050728\n",
            "train loss:0.04986448677207043\n",
            "train loss:0.03998431208599774\n",
            "train loss:0.08315816826887897\n",
            "train loss:0.05221309801629701\n",
            "train loss:0.10367670202457972\n",
            "train loss:0.0758815120255561\n",
            "train loss:0.09346686179229056\n",
            "train loss:0.13556458360686632\n",
            "train loss:0.20707357667913304\n",
            "train loss:0.04514378109415647\n",
            "train loss:0.05868429014807246\n",
            "train loss:0.1041557088848781\n",
            "train loss:0.09063972953027638\n",
            "train loss:0.06961120502591651\n",
            "train loss:0.05083830413388204\n",
            "train loss:0.10398822307703057\n",
            "train loss:0.05979280006431324\n",
            "train loss:0.05221791340315526\n",
            "train loss:0.1037551321025305\n",
            "train loss:0.052987789939231734\n",
            "train loss:0.0707734062741941\n",
            "train loss:0.08428205157663708\n",
            "train loss:0.02617007506708825\n",
            "train loss:0.1392651062261833\n",
            "=== epoch:3, train acc:0.974, test acc:0.96 ===\n",
            "train loss:0.1142333565196613\n",
            "train loss:0.06709309590492991\n",
            "train loss:0.07601230033915195\n",
            "train loss:0.039972742092847256\n",
            "train loss:0.024316311191114513\n",
            "train loss:0.049228811001985714\n",
            "train loss:0.028404298832786452\n",
            "train loss:0.03613002510635812\n",
            "train loss:0.0579902931933343\n",
            "train loss:0.03305452961607927\n",
            "train loss:0.06284288174561503\n",
            "train loss:0.045759316850739494\n",
            "train loss:0.10426972319909898\n",
            "train loss:0.024090942816294023\n",
            "train loss:0.17474150352676496\n",
            "train loss:0.03548199505883148\n",
            "train loss:0.03680508070101551\n",
            "train loss:0.021167416429683526\n",
            "train loss:0.03826961191266667\n",
            "train loss:0.021861216121808358\n",
            "train loss:0.08011287056697357\n",
            "train loss:0.014842829493335697\n",
            "train loss:0.01376752959306091\n",
            "train loss:0.04030950842125702\n",
            "train loss:0.04166772087801642\n",
            "train loss:0.06057703863274361\n",
            "train loss:0.018597703841323848\n",
            "train loss:0.11522335253195136\n",
            "train loss:0.04083910905940839\n",
            "train loss:0.03147646385201478\n",
            "train loss:0.009544855044836542\n",
            "train loss:0.03281558374177244\n",
            "train loss:0.08178750125200705\n",
            "train loss:0.02876848271871836\n",
            "train loss:0.03830011465918178\n",
            "train loss:0.04259652379548731\n",
            "train loss:0.032907621172811395\n",
            "train loss:0.08960415543984432\n",
            "train loss:0.03251558871944386\n",
            "train loss:0.046858737801896685\n",
            "train loss:0.0344096043696466\n",
            "train loss:0.021081631625898822\n",
            "train loss:0.05406495981046963\n",
            "train loss:0.017646959682931805\n",
            "train loss:0.02184437665447673\n",
            "train loss:0.05297615185906465\n",
            "train loss:0.029518224895743513\n",
            "train loss:0.04356943320059549\n",
            "train loss:0.02072260469755939\n",
            "train loss:0.04415730406608911\n",
            "=== epoch:4, train acc:0.982, test acc:0.961 ===\n",
            "train loss:0.014677034239725882\n",
            "train loss:0.09470837656858538\n",
            "train loss:0.04896730507583341\n",
            "train loss:0.0593191240421752\n",
            "train loss:0.03736675974589878\n",
            "train loss:0.040959261945029884\n",
            "train loss:0.06145911449172707\n",
            "train loss:0.03601420812632112\n",
            "train loss:0.03834404127826554\n",
            "train loss:0.044506924578919166\n",
            "train loss:0.03734489153035753\n",
            "train loss:0.013677997276514649\n",
            "train loss:0.012981718267721527\n",
            "train loss:0.10106845859747417\n",
            "train loss:0.027328332632561875\n",
            "train loss:0.09858596499789195\n",
            "train loss:0.06027250422877751\n",
            "train loss:0.06850799007973282\n",
            "train loss:0.05538420723254232\n",
            "train loss:0.08606953324698882\n",
            "train loss:0.01661179215987736\n",
            "train loss:0.028363531490042993\n",
            "train loss:0.021288186680564695\n",
            "train loss:0.053066646456291\n",
            "train loss:0.02332603447208743\n",
            "train loss:0.07255855049132688\n",
            "train loss:0.027046465761993953\n",
            "train loss:0.02199344917687308\n",
            "train loss:0.034082291459837044\n",
            "train loss:0.00971081034212548\n",
            "train loss:0.021024146970428717\n",
            "train loss:0.039093747817846095\n",
            "train loss:0.018411173709910543\n",
            "train loss:0.09143561511947691\n",
            "train loss:0.05244078829657754\n",
            "train loss:0.012845626937448468\n",
            "train loss:0.029143710583121072\n",
            "train loss:0.024845977711976677\n",
            "train loss:0.07196936613988517\n",
            "train loss:0.07088620278173687\n",
            "train loss:0.02121486300434581\n",
            "train loss:0.036848709109984855\n",
            "train loss:0.03158603115749774\n",
            "train loss:0.03270891193473725\n",
            "train loss:0.009986241291206286\n",
            "train loss:0.048787791669830424\n",
            "train loss:0.02690934311951566\n",
            "train loss:0.00833007458778003\n",
            "train loss:0.047962222393975094\n",
            "train loss:0.021304026518410787\n",
            "=== epoch:5, train acc:0.984, test acc:0.964 ===\n",
            "train loss:0.014192330262300865\n",
            "train loss:0.01276759592436034\n",
            "train loss:0.029138482217184732\n",
            "train loss:0.036107445726651546\n",
            "train loss:0.003007678136723497\n",
            "train loss:0.043534529239126686\n",
            "train loss:0.03100184472635065\n",
            "train loss:0.017200796527119616\n",
            "train loss:0.015863613432345606\n",
            "train loss:0.008996391103144284\n",
            "train loss:0.011669132061636755\n",
            "train loss:0.04642194844703944\n",
            "train loss:0.01734785511846831\n",
            "train loss:0.006593897940457036\n",
            "train loss:0.05178395773467747\n",
            "train loss:0.019133308154969896\n",
            "train loss:0.021951196524317346\n",
            "train loss:0.00776982448461233\n",
            "train loss:0.014319396801173024\n",
            "train loss:0.041080949711350366\n",
            "train loss:0.011065049255690289\n",
            "train loss:0.008754182300118696\n",
            "train loss:0.03122515112539384\n",
            "train loss:0.012828311732581836\n",
            "train loss:0.02828007651325355\n",
            "train loss:0.025999330450701318\n",
            "train loss:0.0068458612540294814\n",
            "train loss:0.009810207921683191\n",
            "train loss:0.004818627251704221\n",
            "train loss:0.010931139495307884\n",
            "train loss:0.03209254042471768\n",
            "train loss:0.016525077397316183\n",
            "train loss:0.004604566557018247\n",
            "train loss:0.0052997706589471925\n",
            "train loss:0.00891721810762736\n",
            "train loss:0.013091445437751884\n",
            "train loss:0.00831876131599269\n",
            "train loss:0.002683066682914061\n",
            "train loss:0.018679162080174748\n",
            "train loss:0.00894334701705832\n",
            "train loss:0.011393111269417423\n",
            "train loss:0.002500866888538715\n",
            "train loss:0.005945123336822014\n",
            "train loss:0.03012403614750525\n",
            "train loss:0.009314997517580113\n",
            "train loss:0.00789515535081422\n",
            "train loss:0.0014528397045579053\n",
            "train loss:0.005388398736055392\n",
            "train loss:0.010560462838047673\n",
            "train loss:0.00352120539165578\n",
            "=== epoch:6, train acc:0.995, test acc:0.971 ===\n",
            "train loss:0.0069198746766512675\n",
            "train loss:0.003760167712553998\n",
            "train loss:0.021300463334120258\n",
            "train loss:0.03800852076018009\n",
            "train loss:0.0016138454092800402\n",
            "train loss:0.0015681339505998868\n",
            "train loss:0.004375129000146093\n",
            "train loss:0.0059709221249113755\n",
            "train loss:0.004398805102619043\n",
            "train loss:0.015464711938327656\n",
            "train loss:0.01652848450757962\n",
            "train loss:0.019005655375958153\n",
            "train loss:0.008511381659983476\n",
            "train loss:0.012599019932070337\n",
            "train loss:0.015464178470113499\n",
            "train loss:0.00840227601436585\n",
            "train loss:0.018605897867250743\n",
            "train loss:0.0020742991334395805\n",
            "train loss:0.013351146096032503\n",
            "train loss:0.011571763344029042\n",
            "train loss:0.0026612803475707875\n",
            "train loss:0.0032249771207814677\n",
            "train loss:0.014808130585379555\n",
            "train loss:0.020716062965443493\n",
            "train loss:0.040578412860289646\n",
            "train loss:0.007765223821220133\n",
            "train loss:0.006626494619990489\n",
            "train loss:0.0038458624009283866\n",
            "train loss:0.004161028124285524\n",
            "train loss:0.003664398745083009\n",
            "train loss:0.0056203366989009485\n",
            "train loss:0.01645417940086369\n",
            "train loss:0.010829814616052818\n",
            "train loss:0.00690527964129086\n",
            "train loss:0.0022270715752483725\n",
            "train loss:0.013497759351522592\n",
            "train loss:0.03514725515242738\n",
            "train loss:0.0013517765452344455\n",
            "train loss:0.004089227698996242\n",
            "train loss:0.0024456104098339575\n",
            "train loss:0.0071030009862819545\n",
            "train loss:0.010349482664973654\n",
            "train loss:0.01130886169792627\n",
            "train loss:0.0064423315731173235\n",
            "train loss:0.0023707265014496545\n",
            "train loss:0.03385427986046127\n",
            "train loss:0.006593327511542702\n",
            "train loss:0.011325905561645129\n",
            "train loss:0.009855683316083809\n",
            "train loss:0.008546223546760354\n",
            "=== epoch:7, train acc:0.995, test acc:0.966 ===\n",
            "train loss:0.0060709826137902625\n",
            "train loss:0.0068954284313905365\n",
            "train loss:0.01393942075240367\n",
            "train loss:0.01242775253021004\n",
            "train loss:0.009071998711539403\n",
            "train loss:0.0034605794078535544\n",
            "train loss:0.0342361830865435\n",
            "train loss:0.0035505057633841053\n",
            "train loss:0.0038714228988040705\n",
            "train loss:0.000710160284482329\n",
            "train loss:0.008205362742010376\n",
            "train loss:0.010908254380427876\n",
            "train loss:0.006893485491596055\n",
            "train loss:0.004675206481552182\n",
            "train loss:0.0032577526195692696\n",
            "train loss:0.002612294463661669\n",
            "train loss:0.023341860102357823\n",
            "train loss:0.005920559853570126\n",
            "train loss:0.010978041906071705\n",
            "train loss:0.003702369311656704\n",
            "train loss:0.004673405627143285\n",
            "train loss:0.0048296965519619315\n",
            "train loss:0.006053920343116842\n",
            "train loss:0.003396480360341524\n",
            "train loss:0.0009737072545662262\n",
            "train loss:0.012879315865876364\n",
            "train loss:0.0019667757978551275\n",
            "train loss:0.00565595162096149\n",
            "train loss:0.0016857338664099325\n",
            "train loss:0.005221222607885222\n",
            "train loss:0.0027724275501587723\n",
            "train loss:0.005267265960173623\n",
            "train loss:0.018703112910307632\n",
            "train loss:0.0031150934747899857\n",
            "train loss:0.0044956371147086135\n",
            "train loss:0.00435436316461524\n",
            "train loss:0.00476939441051573\n",
            "train loss:0.0024694810484615533\n",
            "train loss:0.005877852988364793\n",
            "train loss:0.005193556422115612\n",
            "train loss:0.011131002455575151\n",
            "train loss:0.022378995583372533\n",
            "train loss:0.0036726257693203033\n",
            "train loss:0.006522958319995759\n",
            "train loss:0.0047387582815949045\n",
            "train loss:0.004349355641624192\n",
            "train loss:0.01760703044749545\n",
            "train loss:0.011693778200499312\n",
            "train loss:0.006068321142654257\n",
            "train loss:0.004742064515262029\n",
            "=== epoch:8, train acc:0.989, test acc:0.955 ===\n",
            "train loss:0.007047697406101464\n",
            "train loss:0.008038290354388331\n",
            "train loss:0.005122300961590506\n",
            "train loss:0.007391187107302754\n",
            "train loss:0.0010333655995528714\n",
            "train loss:0.012611419882657742\n",
            "train loss:0.00587086301401577\n",
            "train loss:0.004675332089155893\n",
            "train loss:0.018645658059273065\n",
            "train loss:0.008117670994955832\n",
            "train loss:0.021002567868933446\n",
            "train loss:0.09734997689703367\n",
            "train loss:0.006303106794288902\n",
            "train loss:0.004412370141132485\n",
            "train loss:0.02725330249353394\n",
            "train loss:0.020272863959535772\n",
            "train loss:0.011474458299543678\n",
            "train loss:0.02160383948322658\n",
            "train loss:0.007915159563727583\n",
            "train loss:0.008768933960073857\n",
            "train loss:0.004486807697513942\n",
            "train loss:0.009208515292669795\n",
            "train loss:0.0069151935803265455\n",
            "train loss:0.007953421264108342\n",
            "train loss:0.012879687897061967\n",
            "train loss:0.003475773219977423\n",
            "train loss:0.00757351677569656\n",
            "train loss:0.01148279175981379\n",
            "train loss:0.004356279976467133\n",
            "train loss:0.007514595427500743\n",
            "train loss:0.0040943452735034795\n",
            "train loss:0.003158686282638914\n",
            "train loss:0.0038541237925241827\n",
            "train loss:0.005715005052742028\n",
            "train loss:0.01016263752884965\n",
            "train loss:0.002907451435943826\n",
            "train loss:0.0036894104493402097\n",
            "train loss:0.0014206032470870083\n",
            "train loss:0.0063786734170512985\n",
            "train loss:0.01600840052512158\n",
            "train loss:0.010387454882994538\n",
            "train loss:0.009561322838494915\n",
            "train loss:0.012571754740680898\n",
            "train loss:0.004941209343198508\n",
            "train loss:0.0004943301391381558\n",
            "train loss:0.00295458520772027\n",
            "train loss:0.004588782548251526\n",
            "train loss:0.002500331537277033\n",
            "train loss:0.016794106790055274\n",
            "train loss:0.00764128985290485\n",
            "=== epoch:9, train acc:0.993, test acc:0.959 ===\n",
            "train loss:0.014667643420391528\n",
            "train loss:0.0018372059940291703\n",
            "train loss:0.006266076641319204\n",
            "train loss:0.005570314860439202\n",
            "train loss:0.04780761099922971\n",
            "train loss:0.027098631638596774\n",
            "train loss:0.004188494859171408\n",
            "train loss:0.0012419937316824011\n",
            "train loss:0.055640160941734684\n",
            "train loss:0.006349706514633583\n",
            "train loss:0.008625801784902059\n",
            "train loss:0.0045467522336142974\n",
            "train loss:0.003331798549828915\n",
            "train loss:0.003494057429992669\n",
            "train loss:0.007585090941655239\n",
            "train loss:0.0025150102212378646\n",
            "train loss:0.0033076469825911326\n",
            "train loss:0.0249451246642413\n",
            "train loss:0.013224357075531889\n",
            "train loss:0.003850592220262403\n",
            "train loss:0.00633241742695675\n",
            "train loss:0.0018883562385158131\n",
            "train loss:0.008272414260747658\n",
            "train loss:0.015448512696213612\n",
            "train loss:0.0025056412579018213\n",
            "train loss:0.013536417956031714\n",
            "train loss:0.01333063796373349\n",
            "train loss:0.0036247956943436716\n",
            "train loss:0.002346430272474428\n",
            "train loss:0.006697919513890466\n",
            "train loss:0.01910363608949444\n",
            "train loss:0.009657743618253656\n",
            "train loss:0.003160954431663143\n",
            "train loss:0.02561142938848571\n",
            "train loss:0.008080429642263038\n",
            "train loss:0.010384738490548138\n",
            "train loss:0.007108947410911794\n",
            "train loss:0.002647104990920591\n",
            "train loss:0.0023090531280264325\n",
            "train loss:0.005430373815909036\n",
            "train loss:0.00469119561539696\n",
            "train loss:0.005244798479919314\n",
            "train loss:0.008821693584427257\n",
            "train loss:0.005952054919242413\n",
            "train loss:0.004430460920022352\n",
            "train loss:0.002531959342521245\n",
            "train loss:0.0016853348246969187\n",
            "train loss:0.013378514163940348\n",
            "train loss:0.002132255336067553\n",
            "train loss:0.0034268475827124527\n",
            "=== epoch:10, train acc:0.996, test acc:0.958 ===\n",
            "train loss:0.002708719353662026\n",
            "train loss:0.0032796626733299617\n",
            "train loss:0.005218188483790924\n",
            "train loss:0.013686332354723947\n",
            "train loss:0.002665612357573361\n",
            "train loss:0.006937850753424263\n",
            "train loss:0.005110692982440019\n",
            "train loss:0.0022877032223583378\n",
            "train loss:0.001513284600283141\n",
            "train loss:0.0031781481894911407\n",
            "train loss:0.006698168492010561\n",
            "train loss:0.0026441165451176135\n",
            "train loss:0.0020893214365316467\n",
            "train loss:0.0017832138181237463\n",
            "train loss:0.007599191766402785\n",
            "train loss:0.010853844272074203\n",
            "train loss:0.0029752782293718653\n",
            "train loss:0.002993919450231958\n",
            "train loss:0.0018158986013311587\n",
            "train loss:0.0011402738620885468\n",
            "train loss:0.0018564960262623567\n",
            "train loss:0.001291814589842643\n",
            "train loss:0.004633786732668618\n",
            "train loss:0.002777411797431561\n",
            "train loss:0.0073372825496722935\n",
            "train loss:0.003910188775276868\n",
            "train loss:0.000938407518445295\n",
            "train loss:0.005215009921136288\n",
            "train loss:0.0016563197770277063\n",
            "train loss:0.004146808344124494\n",
            "train loss:0.0013854288514391477\n",
            "train loss:0.006177704920399227\n",
            "train loss:0.0008917494599388893\n",
            "train loss:0.0003783702864475923\n",
            "train loss:0.002507931363530578\n",
            "train loss:0.013245550616169223\n",
            "train loss:0.0012398696823294295\n",
            "train loss:0.0029599779335529753\n",
            "train loss:0.00039434972162034595\n",
            "train loss:0.0012938476828357626\n",
            "train loss:0.0009508186402999673\n",
            "train loss:0.001272873621666231\n",
            "train loss:0.0016893427690166574\n",
            "train loss:0.0019351757519548462\n",
            "train loss:0.0022671842928610707\n",
            "train loss:0.002747558936388429\n",
            "train loss:0.002080348545937593\n",
            "train loss:0.0005869771065967862\n",
            "train loss:0.001577598174326335\n",
            "train loss:0.003173330163614221\n",
            "=== epoch:11, train acc:0.992, test acc:0.961 ===\n",
            "train loss:0.0028876804919091724\n",
            "train loss:0.001351883940501894\n",
            "train loss:0.0012501100408487454\n",
            "train loss:0.0012794383700316248\n",
            "train loss:0.0021774458405113732\n",
            "train loss:0.0035119290771355905\n",
            "train loss:0.0032656708482813133\n",
            "train loss:0.004229349461460733\n",
            "train loss:0.005562243539105858\n",
            "train loss:0.0015571990317678923\n",
            "train loss:0.001635878823748646\n",
            "train loss:0.0038908831452417305\n",
            "train loss:0.003092061949513487\n",
            "train loss:0.019594891525599435\n",
            "train loss:0.0018113936882519178\n",
            "train loss:0.0020539805388714153\n",
            "train loss:0.0012842015391458\n",
            "train loss:0.0020813285892569582\n",
            "train loss:0.0023468246203550607\n",
            "train loss:0.0101163851368358\n",
            "train loss:0.00582655455825353\n",
            "train loss:0.008200381005966192\n",
            "train loss:0.018428418432933292\n",
            "train loss:0.0020639977799328858\n",
            "train loss:0.0013947941853442091\n",
            "train loss:0.002074751437353154\n",
            "train loss:0.0010585943680659349\n",
            "train loss:0.0050857001563374945\n",
            "train loss:0.003450316267096527\n",
            "train loss:0.0007823913730152328\n",
            "train loss:0.001864380927877356\n",
            "train loss:0.0004886898391260182\n",
            "train loss:0.0006123739468973507\n",
            "train loss:0.0010460723952850966\n",
            "train loss:0.0005408663759467493\n",
            "train loss:0.0006976463436982386\n",
            "train loss:0.00311521915796717\n",
            "train loss:0.0002136526436987395\n",
            "train loss:0.0017645832906857818\n",
            "train loss:0.007189596456139005\n",
            "train loss:0.00196054296090198\n",
            "train loss:0.0024857147475327596\n",
            "train loss:0.0021474832684672397\n",
            "train loss:0.004894814226058486\n",
            "train loss:0.003700147954326512\n",
            "train loss:0.002976378212179187\n",
            "train loss:0.0009624090904141868\n",
            "train loss:0.0022170877888373717\n",
            "train loss:0.0012070758736661357\n",
            "train loss:0.0037790233985594596\n",
            "=== epoch:12, train acc:0.995, test acc:0.957 ===\n",
            "train loss:0.0020827895024377726\n",
            "train loss:0.003131980433638914\n",
            "train loss:0.01154214240411357\n",
            "train loss:0.006669603905198806\n",
            "train loss:0.0004767061038119722\n",
            "train loss:0.012619484316647507\n",
            "train loss:0.0024654183944940043\n",
            "train loss:0.004276696147280281\n",
            "train loss:0.0037499300194056245\n",
            "train loss:0.004135795604451409\n",
            "train loss:0.0033330915585154503\n",
            "train loss:0.002651422531826703\n",
            "train loss:0.0014750358158890752\n",
            "train loss:0.005135508925204364\n",
            "train loss:0.006405199668760313\n",
            "train loss:0.005186748445203648\n",
            "train loss:0.004607756577850393\n",
            "train loss:0.004143378257673083\n",
            "train loss:0.002113992341627212\n",
            "train loss:0.0021263683305376866\n",
            "train loss:0.005128993552368033\n",
            "train loss:0.004938638145291799\n",
            "train loss:0.0003471735992058136\n",
            "train loss:0.005944112783261906\n",
            "train loss:0.0022103116154390283\n",
            "train loss:0.0020898475862387994\n",
            "train loss:0.001759566918804129\n",
            "train loss:0.020549302320466426\n",
            "train loss:0.0015465224099605564\n",
            "train loss:0.0037953309264656477\n",
            "train loss:0.0021980474719184508\n",
            "train loss:0.004579774532035616\n",
            "train loss:0.004713266786518741\n",
            "train loss:0.002298661996977773\n",
            "train loss:0.0028956421583864692\n",
            "train loss:0.0012755672796864877\n",
            "train loss:0.0019065230093219016\n",
            "train loss:0.0033146057756170844\n",
            "train loss:0.0025498658685907598\n",
            "train loss:0.0006585869079724037\n",
            "train loss:0.002738993423954737\n",
            "train loss:0.002808736859436509\n",
            "train loss:0.0009653628272317813\n",
            "train loss:0.0006610644709936789\n",
            "train loss:0.0018278168412387411\n",
            "train loss:0.013342923817624459\n",
            "train loss:0.002653863881294928\n",
            "train loss:0.0005295143031005887\n",
            "train loss:0.0005346624753516889\n",
            "train loss:0.0020882096353716105\n",
            "=== epoch:13, train acc:0.997, test acc:0.963 ===\n",
            "train loss:0.0013446973681977982\n",
            "train loss:0.0011723094410758922\n",
            "train loss:0.0002683967378797632\n",
            "train loss:0.0008629608336058459\n",
            "train loss:0.0032066623820833045\n",
            "train loss:0.0030661145156410698\n",
            "train loss:0.00273491751112307\n",
            "train loss:0.004447686366825585\n",
            "train loss:0.0035792665915589383\n",
            "train loss:0.001313628880079581\n",
            "train loss:0.0027691667489623296\n",
            "train loss:0.00036390057021914535\n",
            "train loss:0.01097892045925974\n",
            "train loss:0.0009648937784559144\n",
            "train loss:0.0006398612634598837\n",
            "train loss:0.001701189060801336\n",
            "train loss:0.003889616280331681\n",
            "train loss:0.034072418662932764\n",
            "train loss:0.0012349286405466916\n",
            "train loss:0.0048397857137184195\n",
            "train loss:0.0010134424095087733\n",
            "train loss:0.002096860625944234\n",
            "train loss:0.0020382051760653953\n",
            "train loss:0.00045619837567243673\n",
            "train loss:0.001444114403957164\n",
            "train loss:0.0016109062986958461\n",
            "train loss:0.0004837342583366384\n",
            "train loss:0.0009089623903626839\n",
            "train loss:0.0022300815670045562\n",
            "train loss:0.0012597702711769684\n",
            "train loss:0.0006599916291700778\n",
            "train loss:0.0014165439906699097\n",
            "train loss:0.00038587501623177074\n",
            "train loss:0.00037846775158579746\n",
            "train loss:0.00024704177904401613\n",
            "train loss:0.0026986286684073297\n",
            "train loss:0.002359808065432312\n",
            "train loss:0.00029814932766822366\n",
            "train loss:0.0010237190894000334\n",
            "train loss:0.016947857781837546\n",
            "train loss:0.0026153526505447245\n",
            "train loss:0.000144576975171009\n",
            "train loss:0.0010182504597294028\n",
            "train loss:0.0014746092346166878\n",
            "train loss:0.0020299538302313803\n",
            "train loss:0.0006266816947392799\n",
            "train loss:0.00034390414980822135\n",
            "train loss:0.0013608988731750673\n",
            "train loss:0.0004682365377681717\n",
            "train loss:0.0006663997801371401\n",
            "=== epoch:14, train acc:0.998, test acc:0.967 ===\n",
            "train loss:0.0007060291986323527\n",
            "train loss:0.0009514084939495885\n",
            "train loss:0.0002295077382998369\n",
            "train loss:0.0015241762859083212\n",
            "train loss:0.002868321144043059\n",
            "train loss:0.0018751233209485035\n",
            "train loss:0.0005177506026567989\n",
            "train loss:0.0008770132035535532\n",
            "train loss:0.00021419350577382278\n",
            "train loss:0.001109083662401538\n",
            "train loss:0.0011111010542072218\n",
            "train loss:0.002504108556302917\n",
            "train loss:0.0013515380745485195\n",
            "train loss:0.0006627955580462033\n",
            "train loss:0.000938543657287583\n",
            "train loss:0.0001361294833908593\n",
            "train loss:0.0005799123480869322\n",
            "train loss:0.0007131423108770144\n",
            "train loss:0.0009981694345487268\n",
            "train loss:0.01935454690003459\n",
            "train loss:0.0003190846303749447\n",
            "train loss:0.0006309644030144012\n",
            "train loss:0.00025310065334078737\n",
            "train loss:0.0009078170345558786\n",
            "train loss:6.26758481070614e-05\n",
            "train loss:0.0002885639580066925\n",
            "train loss:0.0010443086152549367\n",
            "train loss:0.0010606340565099462\n",
            "train loss:0.0004892464009668612\n",
            "train loss:0.00022230514103888257\n",
            "train loss:0.0008089568573185682\n",
            "train loss:0.0008252129714049688\n",
            "train loss:5.091316571207832e-05\n",
            "train loss:0.0015430229109971432\n",
            "train loss:0.00012132179307537889\n",
            "train loss:0.0014055080872035575\n",
            "train loss:0.0018965901083615167\n",
            "train loss:0.001367697107565773\n",
            "train loss:0.0011694058617409216\n",
            "train loss:0.00028785974255137755\n",
            "train loss:0.000653638186603962\n",
            "train loss:8.792448670120246e-05\n",
            "train loss:0.0004104941123986\n",
            "train loss:0.0005179153621614491\n",
            "train loss:4.604718834662922e-05\n",
            "train loss:0.0002792701757719249\n",
            "train loss:0.00015919320556400404\n",
            "train loss:0.00014678255020474695\n",
            "train loss:0.00024835637712375126\n",
            "train loss:0.0002128415886907879\n",
            "=== epoch:15, train acc:0.999, test acc:0.969 ===\n",
            "train loss:0.0003547010117513268\n",
            "train loss:0.00026026335668194234\n",
            "train loss:0.0005661712632355322\n",
            "train loss:2.635030016284135e-05\n",
            "train loss:0.0003228515520566286\n",
            "train loss:0.0002338340303933577\n",
            "train loss:0.0001425684709161845\n",
            "train loss:3.3981158878703005e-05\n",
            "train loss:0.01200260360157087\n",
            "train loss:0.00027284175440665847\n",
            "train loss:0.0003807684062827786\n",
            "train loss:0.0004250285554422189\n",
            "train loss:0.0001251060514256108\n",
            "train loss:0.00014448931775527297\n",
            "train loss:0.00033900557310533914\n",
            "train loss:0.00024639448536454666\n",
            "train loss:0.00458471970261299\n",
            "train loss:0.00020894257774127563\n",
            "train loss:0.00010208861379187986\n",
            "train loss:8.505781260977328e-05\n",
            "train loss:0.0011774968837857665\n",
            "train loss:0.0005442004432161382\n",
            "train loss:0.0038181437425872226\n",
            "train loss:0.0002861194220708326\n",
            "train loss:0.00026795608129387123\n",
            "train loss:0.00023475674330546376\n",
            "train loss:0.0002742487359331264\n",
            "train loss:9.986280764589507e-05\n",
            "train loss:0.00024209502127852214\n",
            "train loss:0.00037534899132935027\n",
            "train loss:0.0002336383027249005\n",
            "train loss:0.0002588532171361635\n",
            "train loss:0.00024926321800119376\n",
            "train loss:0.001825900188913109\n",
            "train loss:0.0013229967861687786\n",
            "train loss:0.00200421675474844\n",
            "train loss:7.592533424861089e-05\n",
            "train loss:0.00019377023159660326\n",
            "train loss:2.4773921513806758e-05\n",
            "train loss:0.0008083080804860414\n",
            "train loss:0.0003154920290736498\n",
            "train loss:0.0003579460287010402\n",
            "train loss:0.00032477442412251955\n",
            "train loss:0.0013304096860277286\n",
            "train loss:0.0027985756861597377\n",
            "train loss:0.0005479674900699576\n",
            "train loss:0.00017520678187005884\n",
            "train loss:0.0013104034051003913\n",
            "train loss:0.000562009726259773\n",
            "train loss:0.00028189008651450484\n",
            "=== epoch:16, train acc:1.0, test acc:0.964 ===\n",
            "train loss:0.0002531284344178496\n",
            "train loss:3.796193337962924e-05\n",
            "train loss:0.00031718836681850964\n",
            "train loss:0.0015580202645783165\n",
            "train loss:3.2966226614161385e-05\n",
            "train loss:0.00023477014729419965\n",
            "train loss:1.620722103663103e-05\n",
            "train loss:0.00017350804693141219\n",
            "train loss:5.857048801968667e-05\n",
            "train loss:2.4294615528873573e-05\n",
            "train loss:0.000554760654133989\n",
            "train loss:0.0005631109108906359\n",
            "train loss:0.00012534966459265497\n",
            "train loss:6.603413199081793e-05\n",
            "train loss:0.00020216604779429295\n",
            "train loss:0.00024741903041454184\n",
            "train loss:4.704190589277755e-05\n",
            "train loss:7.746645243359895e-05\n",
            "train loss:0.0001092431331453854\n",
            "train loss:0.00014904664715809324\n",
            "train loss:0.00011093590921305182\n",
            "train loss:0.0005380027352351533\n",
            "train loss:3.189158948169497e-05\n",
            "train loss:5.433817376541479e-05\n",
            "train loss:0.00018092118936222649\n",
            "train loss:2.459399831672541e-05\n",
            "train loss:0.0001425096994803003\n",
            "train loss:0.00012331233661147184\n",
            "train loss:5.373513134997835e-05\n",
            "train loss:0.0004496142957021993\n",
            "train loss:3.553702609218062e-05\n",
            "train loss:4.0546878898226334e-05\n",
            "train loss:0.0001049365548604729\n",
            "train loss:0.0007554753072406644\n",
            "train loss:1.572982398143844e-05\n",
            "train loss:5.755610843587321e-05\n",
            "train loss:3.3197308279635266e-05\n",
            "train loss:5.203404382134941e-05\n",
            "train loss:5.433446319880325e-05\n",
            "train loss:2.3460549386134427e-05\n",
            "train loss:7.55109102710809e-05\n",
            "train loss:8.039103750779943e-05\n",
            "train loss:0.00014918006235026642\n",
            "train loss:6.406852880245875e-05\n",
            "train loss:0.0001614884606381616\n",
            "train loss:5.1091188479829565e-05\n",
            "train loss:0.0001506060403938965\n",
            "train loss:0.0017637714219403528\n",
            "train loss:5.000380778469062e-05\n",
            "train loss:8.478693248511042e-05\n",
            "=== epoch:17, train acc:1.0, test acc:0.967 ===\n",
            "train loss:0.00012843284360611576\n",
            "train loss:0.00010598516943608431\n",
            "train loss:0.0008096052378380474\n",
            "train loss:0.00012883785902386606\n",
            "train loss:3.9158609577688504e-05\n",
            "train loss:0.00023877046177622267\n",
            "train loss:0.0002251266770973986\n",
            "train loss:0.00022989029706133949\n",
            "train loss:5.838710118664923e-06\n",
            "train loss:0.0010382235995484714\n",
            "train loss:0.00013206441526707294\n",
            "train loss:2.032777383826967e-05\n",
            "train loss:0.0004917891447959126\n",
            "train loss:0.001389068051613216\n",
            "train loss:0.0015500373778489765\n",
            "train loss:0.00037395674427421913\n",
            "train loss:0.0002674699932198145\n",
            "train loss:0.00025611317519699177\n",
            "train loss:0.000625663485048985\n",
            "train loss:0.00031273598147619504\n",
            "train loss:0.0004889659754396236\n",
            "train loss:7.215987287532027e-05\n",
            "train loss:0.0024010390969104872\n",
            "train loss:0.00012361529305578446\n",
            "train loss:0.00036162669866203056\n",
            "train loss:0.0004967386866295574\n",
            "train loss:0.0006008859337186779\n",
            "train loss:0.0004757184257301839\n",
            "train loss:0.0013292726353791898\n",
            "train loss:0.0008927325371420404\n",
            "train loss:0.00012787787458306662\n",
            "train loss:0.000872268951918198\n",
            "train loss:0.000489499072923306\n",
            "train loss:0.0006908664070348653\n",
            "train loss:0.00016105842065863428\n",
            "train loss:0.00016099075364717975\n",
            "train loss:0.000249162840694921\n",
            "train loss:0.001171276919519583\n",
            "train loss:0.0017694166165826428\n",
            "train loss:0.000989579514343665\n",
            "train loss:0.0005879841413649149\n",
            "train loss:0.0034862694688156874\n",
            "train loss:0.0003797753698692378\n",
            "train loss:0.00041892293379195756\n",
            "train loss:0.001131573870098085\n",
            "train loss:0.0021406997741131696\n",
            "train loss:0.0007782201341236329\n",
            "train loss:0.002192664925450737\n",
            "train loss:0.0009433993514568643\n",
            "train loss:0.0004726355848434391\n",
            "=== epoch:18, train acc:0.999, test acc:0.963 ===\n",
            "train loss:0.0003941809761568376\n",
            "train loss:0.0030303904154020318\n",
            "train loss:0.0007235301157991318\n",
            "train loss:0.00023134747267798922\n",
            "train loss:0.0007238679551015037\n",
            "train loss:0.00014755353067370005\n",
            "train loss:0.00019553855102449164\n",
            "train loss:1.6481280930889883e-05\n",
            "train loss:0.00026962077141606903\n",
            "train loss:0.0009505230480976277\n",
            "train loss:0.00017799511913558206\n",
            "train loss:7.744017464118359e-05\n",
            "train loss:0.000101995528004636\n",
            "train loss:0.000123948125725496\n",
            "train loss:0.0005945808423803571\n",
            "train loss:0.0002481677140240406\n",
            "train loss:0.00018498134027702788\n",
            "train loss:5.619373916007345e-05\n",
            "train loss:5.042340705375897e-05\n",
            "train loss:0.0016059043235368729\n",
            "train loss:8.722925336500723e-05\n",
            "train loss:0.000583696429221716\n",
            "train loss:0.0017958734820982306\n",
            "train loss:0.0001045452769587505\n",
            "train loss:0.00023879224970754936\n",
            "train loss:5.990204922493354e-05\n",
            "train loss:0.00017131222925165953\n",
            "train loss:0.00013115746553943024\n",
            "train loss:7.288424858224752e-05\n",
            "train loss:0.0001684994491265337\n",
            "train loss:0.00013812881493553182\n",
            "train loss:0.0001300031185665916\n",
            "train loss:0.0011320564369619714\n",
            "train loss:0.002869377141808161\n",
            "train loss:0.0008056784039710953\n",
            "train loss:0.002773377137347656\n",
            "train loss:0.0003058597604609419\n",
            "train loss:0.00047804041154511347\n",
            "train loss:0.0001556093506942711\n",
            "train loss:0.0019134734350569598\n",
            "train loss:0.00017562347431076555\n",
            "train loss:0.0001393852798828877\n",
            "train loss:0.002879726173771957\n",
            "train loss:0.00019723454754529235\n",
            "train loss:4.656668082359576e-05\n",
            "train loss:0.0010475391577058858\n",
            "train loss:0.00013934540568110424\n",
            "train loss:0.000310440264133805\n",
            "train loss:0.00036103733923003244\n",
            "train loss:4.68216299972651e-05\n",
            "=== epoch:19, train acc:0.999, test acc:0.972 ===\n",
            "train loss:0.00014187484445851188\n",
            "train loss:0.00034992611236601577\n",
            "train loss:0.0004645558631425853\n",
            "train loss:0.0006447509288622427\n",
            "train loss:0.0007669750389631846\n",
            "train loss:0.0008099338938158182\n",
            "train loss:4.713397410607236e-05\n",
            "train loss:0.0008744053751941338\n",
            "train loss:0.0006026058165996901\n",
            "train loss:7.995529740457763e-05\n",
            "train loss:5.290662426824698e-05\n",
            "train loss:0.0001536049363196289\n",
            "train loss:0.001950913042125502\n",
            "train loss:0.0005530003914066202\n",
            "train loss:0.0003223060287409487\n",
            "train loss:0.00047811815919726413\n",
            "train loss:0.001146331092065172\n",
            "train loss:0.0006154282833497432\n",
            "train loss:0.0007726557149025626\n",
            "train loss:0.0002541581319412959\n",
            "train loss:0.0008399484673544123\n",
            "train loss:0.0011576846936010542\n",
            "train loss:0.00028099042057929237\n",
            "train loss:0.00021952568177220608\n",
            "train loss:0.0008152277645126394\n",
            "train loss:0.0016460684996835043\n",
            "train loss:0.0004994734805989154\n",
            "train loss:0.00018261281186790417\n",
            "train loss:0.0011740041624421918\n",
            "train loss:0.0011611924610519084\n",
            "train loss:0.0008054720880930063\n",
            "train loss:0.00029929938473847637\n",
            "train loss:0.0021125369031244814\n",
            "train loss:0.00018961210276034085\n",
            "train loss:0.0007246946144460546\n",
            "train loss:0.0014256204934795624\n",
            "train loss:0.001295493195348196\n",
            "train loss:0.000631418520105796\n",
            "train loss:0.0008500325310323407\n",
            "train loss:0.0004785241431437568\n",
            "train loss:0.00025676554818369684\n",
            "train loss:0.0006473341523474144\n",
            "train loss:0.0006127820548337537\n",
            "train loss:9.054037523806841e-05\n",
            "train loss:0.00040737404532748195\n",
            "train loss:2.5556652338363795e-05\n",
            "train loss:0.00039723085272702093\n",
            "train loss:0.0005935682446086214\n",
            "train loss:0.0001696403731796095\n",
            "train loss:0.002319730082384032\n",
            "=== epoch:20, train acc:0.999, test acc:0.964 ===\n",
            "train loss:0.0008177321957480435\n",
            "train loss:0.005813624413636583\n",
            "train loss:0.0007901513249693927\n",
            "train loss:0.004720617733210425\n",
            "train loss:0.0005846357260484129\n",
            "train loss:0.001222174930407316\n",
            "train loss:0.0006357379059195447\n",
            "train loss:0.0013961044328954836\n",
            "train loss:0.00019360334565543798\n",
            "train loss:0.0013001761573794635\n",
            "train loss:0.0001553121171556819\n",
            "train loss:0.0014768916463571385\n",
            "train loss:0.0009396821487711467\n",
            "train loss:0.00023521399948871316\n",
            "train loss:0.0002163417885513854\n",
            "train loss:5.071705265879449e-05\n",
            "train loss:6.617037820958794e-05\n",
            "train loss:0.0026839485559659437\n",
            "train loss:2.433379067369101e-05\n",
            "train loss:0.00026500644845261855\n",
            "train loss:0.0004077378805477307\n",
            "train loss:0.0007718653634845833\n",
            "train loss:0.00020490656900248888\n",
            "train loss:0.0016392163883147356\n",
            "train loss:0.0013367279375050504\n",
            "train loss:0.0014207700228869558\n",
            "train loss:0.0005145720173495016\n",
            "train loss:0.0010306205032060642\n",
            "train loss:0.0007519056291099449\n",
            "train loss:0.00016416826723266803\n",
            "train loss:0.00037171941561468065\n",
            "train loss:0.0004780466595517944\n",
            "train loss:0.0006246785819445035\n",
            "train loss:0.0008358963091088041\n",
            "train loss:8.801991323145216e-05\n",
            "train loss:0.0007461801773354741\n",
            "train loss:0.004356571539852822\n",
            "train loss:0.00014361513159689144\n",
            "train loss:0.00018020030956252133\n",
            "train loss:0.0004945724859009286\n",
            "train loss:2.8102172899713477e-05\n",
            "train loss:0.0009365192987100421\n",
            "train loss:0.0007006761618186814\n",
            "train loss:0.00026130113119606256\n",
            "train loss:0.0002034101490170073\n",
            "train loss:0.002098835537442326\n",
            "train loss:0.00013269427382016813\n",
            "train loss:0.0023024409786346605\n",
            "train loss:0.00027156984061078144\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9558\n",
            "학습에 걸린 시간: 388.66초\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#데이터 읽기\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False) #False 의미?\n",
        "\n",
        "#시간이 오래 걸릴 경우 데이터를 줄인다 일단 오천개 고\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:5000], t_test[:5000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "# 신경망 모델: 학습시킬 대상인 CNN 모델의 설계도(SimpleConvNet 객체)를 불러오는 역할\n",
        "network = SimpleConvNet(input_dim=(1,28,28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.01},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "#최적화 기법 결정: 가중치를 갱신하는 방법을 Adam으로 설정합니다.\n",
        "# Adam은 SGD보다 효율적이고 빠르게 최적의 가중치를 찾도록 도와줍니다.\n",
        "\n",
        "# 1. 'time' 라이브러리를 먼저 import 해야 합니다.\n",
        "import time \n",
        "\n",
        "# 2. 학습 시작 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 3. 모델 학습 실행 (가장 오래 걸리는 부분)\n",
        "trainer.train()  #학습 해주세요~~ 라는 의미\n",
        "\n",
        "# 4. 학습 종료 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 5. 경과 시간 계산 및 출력\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"학습에 걸린 시간: {elapsed_time:.2f}초\") \n",
        "# --- (이후 코드: 정확도 리스트 가져오기, 그래프 그리기) ---\n",
        "\n",
        "train_acc_list = trainer.train_acc_list # 학습 데이터 정확도 리스트\n",
        "test_acc_list = trainer.test_acc_list   # 테스트 데이터 정확도 리스트\n",
        "\n",
        "#매게변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "#그래프 그리기\n",
        "markers = {'train':'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
